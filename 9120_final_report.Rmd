---
title: "Proposal"
author: "Zhuohui Liang/zl2974"
output: pdf_document
fontsize : 10pt
geometry:
  margin=0.6in
header-includes:
  - \usepackage{amsmath}
  - \usepackage{setspace}\singlespacing
  - \usepackage{graphicx}
  - \usepackage{algpseudocode}
  - \usepackage{algorithm}
  - \usepackage{todonotes}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      warning = F,
                      message = F,
                      cache = T)

library(SNFtool)
library(tidyverse)
library(gplots)
library(foreach)
library(doParallel)
library(parallel)
library(ggfortify)
library(lattice)
library(gridExtra)
library(reticulate)
library(survival)

use_python("~/anaconda3/bin/python")

source(here::here("R/DiMSC.R"))
source(here::here("R/PMSC.R"))
source(here::here("R/normalize.R"))

knitr::opts_chunk$set(echo = TRUE)

set.seed(123123)

cl = makePSOCKcluster(6)
registerDoParallel(cl)

simulate_data = function(
  signal = 1,
  sigma = NA,
  n = 50,
  n_var = 20
){
  #if(!is.list(signal)) stop("signal should be a list of mean difference of different signals")
  signal = as.list(signal)
  
  n_group = length(signal)
  
  n_signal = n_var%/%n_group
  
  Mu = do.call(c,lapply(signal, function(x) rep(x,n_signal)))
  
  Mu = c(Mu,rep(0,n_var - length(Mu)))
  
  if (is.na(sigma))
    Sigma = diag(n_var)
  if (!is.matrix(sigma) &
      !is.na(sigma) & is.numeric(sigma))
    Sigma = diag(rep(sigma, n_var))
  
  
  return(MASS::mvrnorm(n = n,
                       mu = Mu,
                       Sigma = Sigma))
}

add_noise = function(data,p_noise,sigma = NULL){
  
  if(p_noise <= 0 ) return(data)
  
  if(is.null(sigma)) sigma = 1
  
  
  n_row = nrow(data)
  
  Mu = rep(0,p_noise)
  
  noise = MASS::mvrnorm(n_row,Mu,sigma*diag(p_noise))
  
  return(cbind(data,noise))
  
}
```

## Introduction

  A biology phenotypes can be represent in a number of layers information: clinical(eletronic health record), methylation level, miRNA expression and etc. Some biological process might be identifiable in one type of biological data but not identifiable in another level.And thus multi-view integration is proposed to enhance the signal in different layers of data.  Similarity Network Fusion(SNF)\cite{wang2014similarity} are proposed to fuse information from different omics data via interactively fusing between similarity matrix and draw cluster result on the fused similarity matrix. Multiview Diversity Matrix put a independent constrain terms on the optimization forcing learned self-representation matrix contains complementary information from different view of data and cluster on the combined complementary similarity matrix. Multiview Non-negative Matrix Factorization is proposed to recursively optimize learned factor loads, factors and consensus cluster result to reach consensus result from different view. 
  
  However, besides SNF is build for biological data, all other methods come from computation vision, thus all has an underlying assumption that all data contains information of same number of clusters to achieve signal boosting. This assumption tends not to hold in biological data. The goal of this report is to test if multiview data integration method can boost clustering result in biological settings.  In specific SNF will be compared with the other two methods with on different scenarios which one data only have information to separate some of the clusters but not all. Information from all data are need to find the true clusters. And also test the clustering performance of each methods when data contribute differently on clustering results.


## Method

### Similarity Network Fusion

Similarity Network based method use the similarity calcuate with some kernel apply to some distance measure. In this report, we will use gaussian kernel and eucludian distance.
$$P(i,j) = exp(\frac{dist(X_i-X_j)}{\sigma})$$

After we calculated the similarity matrix, one can perform spectural clustering on the similarity matrix to get clustering result. The spectural clustering question solves for a lower rank indicator matrix $f$, that $minTr(f(I-P)f^t)~,s.t.~f^tf = I$. 

SNF instead of using a single similarity matrix, it first create another set of similarity matrix only keeping the k-nearest neighbour's edges. 

$$S = knn(P)$$
And use the local similarity matrix S of other data, iteratively updating current data's similarity matrix P, until converge.

$$P_1^{t+1} = S_2^tP_1^tS_2^t$$
By iteratively updating, the infomation of other data fused together.


```{r}
data("Data1")
data("Data2")

Data1 = as.matrix(Data1)
Data2 = as.matrix(Data2)

s1 = affinityMatrix(as.matrix(dist(Data1))) %>% as.matrix()
s2 = affinityMatrix(as.matrix(dist(Data2))) %>% as.matrix()
snf = SNF(list(s1,s2)) %>% as.matrix()
diag(snf) = 0

g1 = levelplot(
  s1,
  main = "data1",
  xlab = NULL,
  ylab = NULL,
  cuts = 8,
  colorkey = FALSE,
  scales = list(x = list(at = NULL),
                y = list(at = NULL))
)
g2 = levelplot(
  s2,
  main = "data2",
  xlab = NULL,
  ylab = NULL,
  cuts = 8,
  colorkey = T,
  scales = list(x = list(at = NULL),
                y = list(at = NULL))
)
g3 = levelplot(
  snf,
  main = "SNF",
  xlab = NULL,
  ylab = NULL,
  cuts = 8,
  colorkey = T,
  scales = list(x = list(at = NULL),
                y = list(at = NULL))
)

grid.arrange(g1,g2,ncol = 2)
plot(g3)
```


### DIMSC

Diversity Induce Mulitview Subspace clustering also perform clustering from similarity matrix. It use so-call "self-expressiveness" $$||X - XZ||$$ to calculate the similarity matrix Z, and perform spectural clustring. In DIMSC, the algorithm simultaneously updating similarity matrix of each data, and also introduce a penalty term penalizes correlation between the similarity from different data view. Such that each similarity matrix learn some information not in the other data. And suming all similarity matrix in the end as a single similarity matrix for other task.

$$argmin_Z\sum_v||X^v-X^vZ^v|| + \alpha Tr(Z^vL^vZ^v) +\beta\mbox{Ind}(Z^1,Z^2,...Z^v)$$

```{r}
a = DiMSC(list(Data1,Data2),w_function = cor,K=2)
grid.arrange(levelplot(a$Z[[1]],cuts=8),
             levelplot(a$Z[[2]],cuts=8),ncol=2)
levelplot(a$A,cuts=8)
```


### PMSC

Partition level multiview subspace clustering is similar to above methods and solve for following loss fuction. 
$$argmin_{Z,F,Y} \sum_v||X^v-X^vZ^v|| + \alpha Tr(F^vZ^vF^{v^T}) + \beta||YY^T-F^vF^{v^T}||+\gamma Sparsity$$

But instead of perform clustering on a single similarity in the end, the method solve for clustering result of each data types during updating and also introduces a common clustering result $Y$ and use the clustering result to help learn similarity matrix. By iteratively updating similarity matrix, clustering of each data types and the common clustering result, the Data information is integrated together.


### Simulation

#### Scenario I

This is a simple simulation to demonstrate how the above mention data integration method works. In this scenario, we have two data, each data have 3 clusters and each cluster has 100 subjects. Each subjects has 500 features, and first 12 features are signal to separate subjects from different clusters. First 4 features from data 1 are sample from N(1,2) for subjects from cluster 1 and N(0,2) for cluster 2,3. Next 4 features cluster 2 from data 1 are sample from N(1,2) and cluster 1,3 are sample from N(0,2) and etc. such that data using the first 4 features, we can separate cluster 1 from 2 and 3 and using next 4 to separate cluster 2 from 3 and so on. The rest  features are sample from N(0,2) as noise. Data 2 are generated with similar framework and the same effect size. 

We compare K-means with SNF, DiMSC and PMSC.

#### scenario II

Three data are generated to represent 4 clusters, in each cluster we have 100 subjects and each subject records 100 features but only the first 10 are signal features that provide information to separate different clusters and the rest of the features are noise sample from N(0,1). 
For data 1, the signal features for cluster 1 are sample from N(1,1) and the other clusters from N(0,1), such that data 1 have 2 separable clusters and cluster 1 can be separated from 2,3 and 4. The other data follow similar design such that data 2 can separate cluster 2 from 1,2,4 and data 3 separate cluster 3 from 1,2 and 4. 


In the second settings, we follow the same design but set all the noise feature to 990 decrease the signal to noise ratio.

I will compare k-means with data concatenation against SNF with spectral clusting and DIMSC with spectral clustering


### Data

TCGAâ€™s Kidney Renal Cell Carcinoma subset including 291 tumor samples. \cite{cancer2016comprehensive} has found experiment result of 2 subtypes of Renal Cell Carcinoma that related to patients survival. We will use patient survival to see the clustering result of the comparing models.
In this report, to integrate data to boost signal, we include:
Methylation at Gene level(Illumina HM450K platform), gene mutation(Somatic mutations at gene level SNV) and RNAseq(HiSeq platform) are selected
for the study for both types of RCC.  Missing observation in the methylation data is assumed to be Missing-at-random and use K-nearest-neighbour method for imputation.

# Result

## Simulation

### Scenario I

We deplay the data designed and the simulation result.

```{r data1}
s1 = rbind(
  simulate_data(c(0, 0, 1), n_var = 48,sigma = 2),
  simulate_data(c(1, 0, 0), n_var = 48,sigma = 2),
  simulate_data(c(0, 1, 0), n_var = 48,sigma = 2)
)


s2 = rbind(
  simulate_data(c(0, 1, 0), n_var = 48,sigma = 2),
  simulate_data(c(0, 0, 1), n_var = 48,sigma = 2),
  simulate_data(c(1, 0, 0), n_var = 48,sigma = 2)
)

data_list = list(s1, s2)

data_list = lapply(data_list, function(x) add_noise(x,500-48,2))

true_label = as.factor(sort(rep(1:3, 50)))

do.call(grid.arrange, c(lapply(data_list, function(x)
  levelplot(t(x), aspect = "fill",cuts = 8)), ncol = 2))

do.call(grid.arrange, c(lapply(data_list, function(x) {
  x = as_tibble(x)
  x$label = true_label
  autoplot(prcomp(x[,-ncol(x)]), data = x, colour = "label")
}), ncol = 2))
```





```{r simulation1}
set.seed(123123)

# pretrain_1 = SNF_tuning(data_list,K=3)
# pretrain_1$NMI = sapply(pretrain_1$cluster,function(x) calNMI(true_label,x))
# pretrain_1[which.max(pretrain_1$NMI),]

sim_1 =
  foreach(i = 1:20,
          .combine = "rbind",
          .packages = c("SNFtool","tidyverse")) %dopar% {
            
            s1 = rbind(
              simulate_data(c(0, 0, 1), n_var = 48, sigma = 2),
              simulate_data(c(1, 0, 0), n_var = 48, sigma = 2),
              simulate_data(c(0, 1, 0), n_var = 48, sigma = 2)
            )
            
            
            s2 = rbind(
              simulate_data(c(0, 1, 0), n_var = 48, sigma = 2),
              simulate_data(c(0, 0, 1), n_var = 48, sigma = 2),
              simulate_data(c(1, 0, 0), n_var = 48, sigma = 2)
            )
            
            data_list = list(s1, s2)
            
            data_list = lapply(data_list, function(x)
              add_noise(x, 500 - 48, 2))
            
            
            result_kmeans = kmeans(do.call(cbind, data_list), 3,
                                   algorithm = "MacQueen")$cluster
            
            result_snf = spectralClustering(SNF(lapply(data_list, function(x) {
              affinityMatrix(as.matrix(dist(x)),
                             K = 10,
                             sigma = 0.45)
            }),
            K = 20,
            t = 20),
            K = 3)
            
            result_PMSC = PMSC(
              data_list,
              K = 3,
              alpha = 100,
              beta = 0.01,
              gamma = 0.001
            )$Y
            
            result_DIMSC = DiMSC(data_list,3,w_function = cor)$cluster
            
            tibble(
              kmeans =calNMI(true_label, result_kmeans),
              SNF = calNMI(true_label, result_snf),
              PMSC = calNMI(true_label, result_PMSC),
              DIMSC = calNMI(true_label,result_DIMSC)
            )
          }
```


```{r sim1reuslt}
sim_1 %>% 
  pivot_longer(everything()) %>% 
  mutate(name = forcats::fct_reorder(name,value)) %>% 
  ggplot(aes(x = name,y = value,colour = name))+
  geom_boxplot()+
  labs(title = "Simulation 2,setting2")
```

From simulation result, we can see that in this settings, data integration methods are better than the data concatenation and k-means method. PMSC is outperform all other methods.

### Scenario II

```{r data2}
s1 = rbind(
  simulate_data(c(1), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10)
)


s2 = rbind(
  simulate_data(c(0), n_var = 10),
  simulate_data(c(1), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10)
)


s3 = rbind(
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(1), n_var = 10),
  simulate_data(c(0), n_var = 10)
)

data_list = list(s1, s2, s3)

data_list = lapply(data_list, function(x) add_noise(x,90))

true_label = as.factor(sort(rep(1:4, 50)))

do.call(grid.arrange, c(lapply(data_list, function(x)
  levelplot(t(x), aspect = "fill",main = "scenario II")), ncol = 2))

do.call(grid.arrange, c(lapply(data_list, function(x) {
  x = as_tibble(x)
  x$label = true_label
  autoplot(prcomp(x[,-ncol(x)]), data = x, colour = "label",main = "Scenario II")
}), ncol = 2))
```

```{r simulation21}

sim_2 = foreach(
  i = 1:20,
  .combine = "rbind",
  .packages = c("SNFtool", "tidyverse")
) %dopar% {
  
  s1 = rbind(
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  
  s2 = rbind(
    simulate_data(c(0), n_var = 10),
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  
  s3 = rbind(
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  data_list = list(s1, s2, s3)
  
  data_list = lapply(data_list, function(x)
    add_noise(x, 90))
  
  true_label = as.factor(sort(rep(1:4, 50)))
  
  result_kmeans = kmeans(do.call(cbind, data_list), 4,
                         algorithm = "MacQueen")$cluster
  
  result_snf = spectralClustering(SNF(lapply(data_list, function(x) {
    affinityMatrix(as.matrix(dist(x)),
                   K = 10,
                   sigma = 0.65)
  }),
  K = 20,
  t = 20),
  K = 4)
  
  result_PMSC = PMSC(
    data_list,
    K = 4,
    alpha = 100,
    beta = 0.01,
    gamma = 0.001
  )$Y
  
  result_DIMSC = DiMSC(data_list, 4, w_function = cor)$cluster
  
  tibble(
    kmeans = calNMI(true_label, result_kmeans),
    SNF = calNMI(true_label, result_snf),
    PMSC = calNMI(true_label, result_PMSC),
    DIMSC = calNMI(true_label, result_DIMSC)
  )
}
```

```{r sim21reuslt}
sim_2 %>% 
  pivot_longer(everything()) %>% 
  mutate(name = forcats::fct_reorder(name,value)) %>% 
  ggplot(aes(x = name,y = value,colour = name))+
  geom_boxplot()
```

In the second simultion, we can see that the data fit our design, and comparing these methods, we can see that both data integration methods designs for computer vision fail in this specific settings. but SNF and PMSC still out-perform data concatenation and k-means. But as we reduce the signal to noise ratio, although they still slightly out-perform kmeans, but the overall NMI is negligible to consider.


#### Settings 2

```{r eval = F,include=F}
set.seed(123123)

pretrain_1 = SNF_tuning(data_list,K=4)
pretrain_1$NMI = sapply(pretrain_1$cluster,function(x) calNMI(true_label,x))
pretrain_1[which.max(pretrain_1$NMI),]
```


```{r simulation22}

sim_3 = foreach(
  i = 1:20,
  .combine = "rbind",
  .packages = c("SNFtool", "tidyverse")
) %dopar% {
  
  s1 = rbind(
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  
  s2 = rbind(
    simulate_data(c(0), n_var = 10),
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  
  s3 = rbind(
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  data_list = list(s1, s2, s3)
  
  data_list = lapply(data_list, function(x)
    add_noise(x, 990))
  
  true_label = as.factor(sort(rep(1:4, 50)))
  
  result_kmeans = kmeans(do.call(cbind, data_list), 4,
                         algorithm = "MacQueen")$cluster
  
  result_snf = spectralClustering(SNF(lapply(data_list, function(x) {
    affinityMatrix(as.matrix(dist(x)),
                   K = 50,
                   sigma = 0.8)
  }),
  K = 20,
  t = 20),
  K = 4)
  
  result_PMSC = PMSC(
    data_list,
    K = 4,
    alpha = 100,
    beta = 0.01,
    gamma = 0.001
  )$Y
  
  result_DIMSC = DiMSC(data_list, 4, w_function = cor)$cluster
  
  tibble(
    kmeans = calNMI(true_label, result_kmeans),
    SNF = calNMI(true_label, result_snf),
    PMSC = calNMI(true_label, result_PMSC),
    DIMSC = calNMI(true_label, result_DIMSC)
  )
}
```

```{r sim22result}
sim_3 %>% 
  pivot_longer(everything()) %>% 
  mutate(name = forcats::fct_reorder(name,value)) %>% 
  ggplot(aes(x = name,y = value,colour = name))+
  geom_boxplot()+
  labs(title = "Simulation 2,setting2")
```


### TCGA result

```{r tcga,echo=F}
mirna = read.csv("data/Human__TCGA_KIRP__BDGSC__miRNASeq__HS_miR__01_28_2016__BI__Gene__Firehose_RPM_log2.cct",sep = "\t")

methylation = read.csv("data/Human__TCGA_KIRP__JHU_USC__Methylation__Meth450__01_28_2016__BI__Gene__Firehose_Methylation_Prepocessor.cct",sep = "\t")

rnaseq = read.csv("data/Human__TCGA_KIRP__UNC__RNAseq__HiSeq_RNA__01_28_2016__BI__Gene__Firehose_RSEM_log2.cct",sep = "\t")

mutation = read.csv("data/Human__TCGA_KIRP__WUSM__Mutation__GAIIx__01_28_2016__BI__Gene__Firehose_MutSig2CV.cbt",sep = "\t")

survival = read.csv("data/Human__TCGA_KIRP__MS__Clinical__Clinical__01_28_2016__BI__Clinical__Firehose.tsi",sep = "\t") 

sample_name = intersect(intersect(intersect(colnames(mirna),colnames(methylation)),
                        colnames(rnaseq)),
                        colnames(mutation))

mirna = t(mirna[,sample_name[-1]])
mutation = t(mutation[,sample_name[-1]])
methylation = t(methylation[,sample_name[-1]])
methylation =
  apply(methylation, 2, function(y) {
    y[is.na(y)] = mean(y, na.rm = T)
    y
  })

rnaseq = t(rnaseq[,sample_name[-1]])

survival = t(survival[,sample_name])
colnames(survival) = survival[1,]
survival = survival[-1,] %>% 
  as_tibble() %>% 
  mutate(subject = rownames(.)) %>% 
  separate(overallsurvival,into = c("time","event"),sep = ",") %>% 
  mutate(across(c(event,time),as.numeric))
```




```{r skmean}
result_kmeans = kmeans(mirna, 3, algorithm = "MacQueen")$cluster

survival$cluster = result_kmeans

autoplot(survfit(Surv(time, event) ~ cluster, data = survival), main = "Single-Kmeans")

sur_skmean = survdiff(Surv(time, event) ~ cluster, data = survival)
```

```{r ckmean}
result_kmeans = kmeans(cbind(mirna, methylation, rnaseq, mutation), 3, algorithm = "MacQueen")$cluster

survival$cluster = result_kmeans

autoplot(survfit(Surv(time, event) ~ cluster, data = survival), main = "Concatenate-Kmeans")

sur_ckmean = survdiff(Surv(time, event) ~ cluster, data = survival)
```


```{r surSNF}

affinity_list = lapply(list(mirna, methylation, rnaseq,mutation),
                       function(x) {
                         affinityMatrix(as.matrix(dist(x)),
                                        K = 30,
                                        sigma = 0.5)
                       })

# plot(1:20,eigen(normalize(SNF(affinity_list,K = 20)))$val[1:20])

result_snf = spectralClustering(SNF(affinity_list,K = 30),3)

survival$cluster = result_snf

autoplot(survfit(Surv(time,event)~cluster,data = survival),main = "SNF")
sur_SNF = survdiff(Surv(time, event) ~ cluster, data = survival)
```


```{r PMSC}
result_PMSC = PMSC(list(mirna,methylation,rnaseq),K=3,beta = 0.01,.max_iter = 20)$Y

survival$cluster = result_PMSC

autoplot(survfit(Surv(time,event)~cluster,data = survival),main = "PMSC")
sur_PMSC = survdiff(Surv(time, event) ~ cluster, data = survival)
```



```{r DIMSC}
result_DIMSC = DiMSC(list(mirna,mutation,rnaseq,methylation),K=3)$cluster

survival$cluster = result_DIMSC

autoplot(survfit(Surv(time,event)~cluster,data = survival),main = "DIMSC")
sur_DIMSC = survdiff(Surv(time, event) ~ cluster, data = survival)
```


```{r sur_comp}
tibble(
  method = c("single kmean", "concatanate kmean", "SNF", "DISMC", "PMSC"),
  log_rank_test = lapply(list(sur_skmean, sur_ckmean, sur_SNF, sur_DIMSC, sur_PMSC), function(x)
    x$chis)
) %>%
  knitr::kable(caption = "Log-Rank test for survival trend")
```

 We choose 3 subtypes based on the maximum eigen gap of the similarity matrix of all 3 data set, and compare the survival of all methods clustering patient into 3 clusters against selecting using only miRNA to perform the clusters. 
 
 We can see that SNF clusters patients into 2 group has better survival and 1 group with worse outcomes. SNF has a more statistically significant survival difference than single kmeans and concatenate-data kmeans. 
 
 DISMC perform poorly in identifing the survival function. PMSC cluster 2 group has worse outcome and 1 group with better outcome.


# Discussion

Data integration can boost signal when the signal is weak in single data types. But we also see that K-mean with single data types can have similar performance when the signal is strong enough to separate all clusters. DIMSC have high performance in all data have similar data structure settings, but fails when this setting is change. We can see that this method without modification might not suitable for omics data's task.

\bibliographystyle{alpha}
\bibliography{scholar}


