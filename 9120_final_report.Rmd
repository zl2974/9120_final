---
title: "Proposal"
author: "Zhuohui Liang/zl2974"
output: pdf_document
fontsize : 10pt
geometry:
  margin=0.6in
header-includes:
  - \usepackage{amsmath}
  - \usepackage{setspace}\singlespacing
  - \usepackage{graphicx}
  - \usepackage{algpseudocode}
  - \usepackage{algorithm}
  - \usepackage{todonotes}
  - \usepackage[backend=biber,style=apa,]{biblatex}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      warning = F,
                      message = F)

library(SNFtool)
library(tidyverse)
library(gplots)
library(foreach)
library(doParallel)
library(parallel)
library(ggfortify)
library(lattice)
library(gridExtra)
library(reticulate)
library(survival)

use_python("~/anaconda3/bin/python")

source(here::here("R/DiMSC.R"))
source(here::here("R/PMSC.R"))
source(here::here("R/normalize.R"))

knitr::opts_chunk$set(echo = TRUE)

set.seed(123123)

cl = makePSOCKcluster(6)
registerDoParallel(cl)

simulate_data = function(
  signal = 1,
  sigma = NA,
  n = 50,
  n_var = 20
){
  #if(!is.list(signal)) stop("signal should be a list of mean difference of different signals")
  signal = as.list(signal)
  
  n_group = length(signal)
  
  n_signal = n_var%/%n_group
  
  Mu = do.call(c,lapply(signal, function(x) rep(x,n_signal)))
  
  Mu = c(Mu,rep(0,n_var - length(Mu)))
  
  if (is.na(sigma))
    Sigma = diag(n_var)
  if (!is.matrix(sigma) &
      !is.na(sigma) & is.numeric(sigma))
    Sigma = diag(rep(sigma, n_var))
  
  
  return(MASS::mvrnorm(n = n,
                       mu = Mu,
                       Sigma = Sigma))
}

add_noise = function(data,p_noise,sigma = NULL){
  
  if(p_noise <= 0 ) return(data)
  
  if(is.null(sigma)) sigma = 1
  
  
  n_row = nrow(data)
  
  Mu = rep(0,p_noise)
  
  noise = MASS::mvrnorm(n_row,Mu,sigma*diag(p_noise))
  
  return(cbind(data,noise))
  
}
```

## Introduction

  A biology phenotypes can be represent in a number of layers information: clinical(eletronic health record), methylation level, miRNA expression and etc. Some biological process might be identifiable in one type of biological data but not identifiable in another level.And thus multi-view integration is proposed to enhance the signal in different layers of data.  Similarity Network Fusion(SNF)\cite{wang2014similarity} are proposed to fuse information from different omics data via interactively fusing between similarity matrix and draw cluster result on the fused similarity matrix. Multiview Diversity Matrix put a independent constrain terms on the optimization forcing learned self-representation matrix contains complementary information from different view of data and cluster on the combined complementary similarity matrix. Multiview Non-negative Matrix Factorization is proposed to recursively optimize learned factor loads, factors and consensus cluster result to reach consensus result from different view. 
  
  However, besides SNF is build for biological data, all other methods come from computation vision, thus all has an underlying assumption that all data contains information of same number of clusters to achieve signal boosting. This assumption tends not to hold in biological data. The goal of this report is to test if multiview data integration method can boost clustering result in biological settings.  In specific SNF will be compared with the other two methods with on different scenarios which one data only have information to separate some of the clusters but not all. Information from all data are need to find the true clusters. And also test the clustering performance of each methods when data contribute differently on clustering results.


## Method

### Similarity Network Fusion


$$P(i,j) = exp(\frac{dist(X_i-X_j)}{\sigma})$$

$$S = knn(P)$$

$$P_1^{t+1} = S_2^tP_1^tS_2^t$$

```{r}
data("Data1")
data("Data2")

Data1 = as.matrix(Data1)
Data2 = as.matrix(Data2)

s1 = affinityMatrix(as.matrix(dist(Data1))) %>% as.matrix()
s2 = affinityMatrix(as.matrix(dist(Data2))) %>% as.matrix()
snf = SNF(list(s1,s2)) %>% as.matrix()
diag(snf) = 0

g1 = levelplot(
  s1,
  main = "data1",
  xlab = NULL,
  ylab = NULL,
  cuts = 8,
  colorkey = FALSE,
  scales = list(x = list(at = NULL),
                y = list(at = NULL))
)
g2 = levelplot(
  s2,
  main = "data2",
  xlab = NULL,
  ylab = NULL,
  cuts = 8,
  colorkey = T,
  scales = list(x = list(at = NULL),
                y = list(at = NULL))
)
g3 = levelplot(
  snf,
  main = "SNF",
  xlab = NULL,
  ylab = NULL,
  cuts = 8,
  colorkey = T,
  scales = list(x = list(at = NULL),
                y = list(at = NULL))
)

grid.arrange(g1,g2,ncol = 2)
plot(g3)
```


### DIMSC

$$argmin_Z\sum_v||X^v-X^vZ^v|| + \alpha Tr(Z^vL^vZ^v) +\beta\mbox{Ind}(Z^1,Z^2,...Z^v)$$

```{r}
a = DiMSC(list(Data1,Data2),w_function = cor,K=2)
grid.arrange(levelplot(a$Z[[1]],cuts=8),
             levelplot(a$Z[[2]],cuts=8),ncol=2)
levelplot(a$A,cuts=8)
```


### PMSC

$$argmin_{Z,F,Y} \sum_v||X^v-X^vZ^v|| + \alpha Tr(F^vZ^vF^{v^T}) + \beta||YY^T-F^vF^{v^T}||+\gamma Sparsity$$

### Simulation

#### Scenario I

This is a simple simulation to demonstrate how the above mention data integration method works. In this scenario, we have two data, each data have 3 clusters and each cluster has 100 subjects. Each subjects has 500 features, and first 12 features are signal to separate subjects from different clusters. First 4 features from data 1 are sample from N(1,2) for subjects from cluster 1 and N(0,2) for cluster 2,3. Next 4 features cluster 2 from data 1 are sample from N(1,2) and cluster 1,3 are sample from N(0,2) and etc. such that data using the first 4 features, we can separate cluster 1 from 2 and 3 and using next 4 to separate cluster 2 from 3 and so on. The rest  features are sample from N(0,2) as noise. Data 2 are generated with similar framework and the same effect size. 

We compare K-means with SNF, DiMSC and PMSC.

#### scenario II

Three data are generated to represent 4 clusters, in each cluster we have 100 subjects and each subject records 100 features but only the first 10 are signal features that provide information to separate different clusters and the rest of the features are noise sample from N(0,1). 
For data 1, the signal features for cluster 1 are sample from N(1,1) and the other clusters from N(0,1), such that data 1 have 2 separable clusters and cluster 1 can be separated from 2,3 and 4. The other data follow similar design such that data 2 can separate cluster 2 from 1,2,4 and data 3 separate cluster 3 from 1,2 and 4. 


In the second settings, we follow the same design but set all the standard deviation to 2 to decrease the effect size.

In the third settings, we further decrease the effect size by setting the standard deviation to 3.

I will compare k-means with data concatenation against SNF with spectral clusting and DIMSC with spectral clustering

#### scenario III

Again we have 3 data and 4 clusters, each cluster has 100 subjects. In this scenario, data 1 has 1000 features, data 2 has 1e+4 and data 3 has 1e+5 features. The first 10% features are set to be signals and the rest as noise sample from N(0,1). we set the signal in each data as scenario I such that all data are needed to find ground truth clustering result. We also test the case when effect size is set as settings 2,3 in scenario I.

# Result

## Simulation

### Scenario I

```{r}
s1 = rbind(
  simulate_data(c(0, 0, 1), n_var = 48,sigma = 2),
  simulate_data(c(1, 0, 0), n_var = 48,sigma = 2),
  simulate_data(c(0, 1, 0), n_var = 48,sigma = 2)
)


s2 = rbind(
  simulate_data(c(0, 1, 0), n_var = 48,sigma = 2),
  simulate_data(c(0, 0, 1), n_var = 48,sigma = 2),
  simulate_data(c(1, 0, 0), n_var = 48,sigma = 2)
)

data_list = list(s1, s2)

data_list = lapply(data_list, function(x) add_noise(x,500-48,2))

true_label = as.factor(sort(rep(1:3, 50)))

do.call(grid.arrange, c(lapply(data_list, function(x)
  levelplot(t(x), aspect = "fill",cuts = 8)), ncol = 2))

do.call(grid.arrange, c(lapply(data_list, function(x) {
  x = as_tibble(x)
  x$label = true_label
  autoplot(prcomp(x[,-ncol(x)]), data = x, colour = "label")
}), ncol = 2))
```





```{r}
set.seed(123123)

# pretrain_1 = SNF_tuning(data_list,K=3)
# pretrain_1$NMI = sapply(pretrain_1$cluster,function(x) calNMI(true_label,x))
# pretrain_1[which.max(pretrain_1$NMI),]

sim_1 =
  foreach(i = 1:20,
          .combine = "rbind",
          .packages = c("SNFtool","tidyverse")) %dopar% {
            
            s1 = rbind(
              simulate_data(c(0, 0, 1), n_var = 48, sigma = 2),
              simulate_data(c(1, 0, 0), n_var = 48, sigma = 2),
              simulate_data(c(0, 1, 0), n_var = 48, sigma = 2)
            )
            
            
            s2 = rbind(
              simulate_data(c(0, 1, 0), n_var = 48, sigma = 2),
              simulate_data(c(0, 0, 1), n_var = 48, sigma = 2),
              simulate_data(c(1, 0, 0), n_var = 48, sigma = 2)
            )
            
            data_list = list(s1, s2)
            
            data_list = lapply(data_list, function(x)
              add_noise(x, 500 - 48, 2))
            
            
            result_kmeans = kmeans(do.call(cbind, data_list), 3,
                                   algorithm = "MacQueen")$cluster
            
            result_snf = spectralClustering(SNF(lapply(data_list, function(x) {
              affinityMatrix(as.matrix(dist(x)),
                             K = 10,
                             sigma = 0.45)
            }),
            K = 20,
            t = 20),
            K = 3)
            
            result_PMSC = PMSC(
              data_list,
              K = 3,
              alpha = 100,
              beta = 0.01,
              gamma = 0.001
            )$Y
            
            result_DIMSC = DiMSC(data_list,3,w_function = cor)$cluster
            
            tibble(
              kmeans =calNMI(true_label, result_kmeans),
              SNF = calNMI(true_label, result_snf),
              PMSC = calNMI(true_label, result_PMSC),
              DIMSC = calNMI(true_label,result_DIMSC)
            )
          }
```


```{r}
sim_1 %>% 
  pivot_longer(everything()) %>% 
  mutate(name = forcats::fct_reorder(name,value)) %>% 
  ggplot(aes(x = name,y = value,colour = name))+
  geom_boxplot()
```


### Scenario II

```{r}
s1 = rbind(
  simulate_data(c(1), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10)
)


s2 = rbind(
  simulate_data(c(0), n_var = 10),
  simulate_data(c(1), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10)
)


s3 = rbind(
  simulate_data(c(0), n_var = 10),
  simulate_data(c(0), n_var = 10),
  simulate_data(c(1), n_var = 10),
  simulate_data(c(0), n_var = 10)
)

data_list = list(s1, s2, s3)

data_list = lapply(data_list, function(x) add_noise(x,90))

true_label = as.factor(sort(rep(1:4, 50)))

do.call(grid.arrange, c(lapply(data_list, function(x)
  levelplot(t(x), aspect = "fill",main = "scenario I")), ncol = 2))

do.call(grid.arrange, c(lapply(data_list, function(x) {
  x = as_tibble(x)
  x$label = true_label
  autoplot(prcomp(x[,-ncol(x)]), data = x, colour = "label",main = "Scenario I")
}), ncol = 2))
```

```{r}

sim_2 = foreach(
  i = 1:20,
  .combine = "rbind",
  .packages = c("SNFtool", "tidyverse")
) %dopar% {
  
  s1 = rbind(
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  
  s2 = rbind(
    simulate_data(c(0), n_var = 10),
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  
  s3 = rbind(
    simulate_data(c(0), n_var = 10),
    simulate_data(c(0), n_var = 10),
    simulate_data(c(1), n_var = 10),
    simulate_data(c(0), n_var = 10)
  )
  
  data_list = list(s1, s2, s3)
  
  data_list = lapply(data_list, function(x)
    add_noise(x, 90))
  
  true_label = as.factor(sort(rep(1:4, 50)))
  
  result_kmeans = kmeans(do.call(cbind, data_list), 4,
                         algorithm = "MacQueen")$cluster
  
  result_snf = spectralClustering(SNF(lapply(data_list, function(x) {
    affinityMatrix(as.matrix(dist(x)),
                   K = 10,
                   sigma = 0.65)
  }),
  K = 20,
  t = 20),
  K = 4)
  
  result_PMSC = PMSC(
    data_list,
    K = 4,
    alpha = 100,
    beta = 0.01,
    gamma = 0.001
  )$Y
  
  result_DIMSC = DiMSC(data_list, 4, w_function = cor)$cluster
  
  tibble(
    kmeans = calNMI(true_label, result_kmeans),
    SNF = calNMI(true_label, result_snf),
    PMSC = calNMI(true_label, result_PMSC),
    DIMSC = calNMI(true_label, result_DIMSC)
  )
}
```

```{r}
sim_2 %>% 
  pivot_longer(everything()) %>% 
  mutate(name = forcats::fct_reorder(name,value)) %>% 
  ggplot(aes(x = name,y = value,colour = name))+
  geom_boxplot()
```
### Scenario III


### TCGA result

```{r}
mirna = read.csv("data/Human__TCGA_KIRP__BDGSC__miRNASeq__HS_miR__01_28_2016__BI__Gene__Firehose_RPM_log2.cct",sep = "\t")

methylation = read.csv("data/Human__TCGA_KIRP__JHU_USC__Methylation__Meth450__01_28_2016__BI__Gene__Firehose_Methylation_Prepocessor.cct",sep = "\t")

rnaseq = read.csv("data/Human__TCGA_KIRP__UNC__RNAseq__HiSeq_RNA__01_28_2016__BI__Gene__Firehose_RSEM_log2.cct",sep = "\t")

mutation = read.csv("data/Human__TCGA_KIRP__WUSM__Mutation__GAIIx__01_28_2016__BI__Gene__Firehose_MutSig2CV.cbt",sep = "\t")

survival = read.csv("data/Human__TCGA_KIRP__MS__Clinical__Clinical__01_28_2016__BI__Clinical__Firehose.tsi",sep = "\t") 

sample_name = intersect(intersect(intersect(colnames(mirna),colnames(methylation)),
                        colnames(rnaseq)),
                        colnames(mutation))

mirna = t(mirna[,sample_name[-1]])
mutation = t(mutation[,sample_name[-1]])
methylation = t(methylation[,sample_name[-1]])
methylation =
  apply(methylation, 2, function(y) {
    y[is.na(y)] = mean(y, na.rm = T)
    y
  })

rnaseq = t(rnaseq[,sample_name[-1]])

survival = t(survival[,sample_name])
colnames(survival) = survival[1,]
survival = survival[-1,] %>% 
  as_tibble() %>% 
  mutate(subject = rownames(.)) %>% 
  separate(overallsurvival,into = c("time","event"),sep = ",") %>% 
  mutate(across(c(event,time),as.numeric))
```


```{r}
# sil_score = c()
# result_kmeans = list()
# for (k in 2:10) {
#   result_kmeans[[k]] = kmeans(cbind(mirna, methylation, rnaseq, mutation), k, algorithm = "MacQueen")$cluster
#   sil_score[k] = mean(cluster::silhouette(result_kmeans[[k]], dist(cbind(mirna, methylation, rnaseq, mutation)))[,3])
# }

# plot(1:10,sil_score)

result_kmeans = kmeans(cbind(mirna, methylation, rnaseq, mutation), 3, algorithm = "MacQueen",nstart = 3)$cluster

survival$cluster = result_kmeans

autoplot(survfit(Surv(time, event) ~ cluster, data = survival), main = "Kmeans")
```



```{r}
affinity_list = lapply(list(mirna, methylation, rnaseq,mutation),
                       function(x) {
                         affinityMatrix(as.matrix(dist(x)),
                                        K = 20,
                                        sigma = 0.5)
                       })

# plot(1:20,eigen(normalize(SNF(affinity_list,K = 20)))$val[1:20])

result_snf = spectralClustering(SNF(affinity_list,K = 20),3)

survival$cluster = result_snf

autoplot(survfit(Surv(time,event)~cluster,data = survival),main = "SNF")
```


```{r}
result_PMSC = PMSC(list(mirna,methylation,rnaseq),K=3,beta = 0.01)$Y

survival$cluster = result_PMSC

autoplot(survfit(Surv(time,event)~cluster,data = survival),main = "PMSC")
```



```{r}
result_DIMSC = DiMSC(list(mirna,mutation,rnaseq,methylation),K=3)$cluster

survival$cluster = result_DIMSC

autoplot(survfit(Surv(time,event)~cluster,data = survival),main = "DIMSC")
```




\bibliographystyle{alpha}
\bibliography{reference}


